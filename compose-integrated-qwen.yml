# Integrated stack: TTS (tts-indic-server) and LLM (vllm-server) in compose; ASR can be host.
# Copy .env.example to .env. UI at http://localhost:80 â€” proxies /v1 to backend.
services:
  talk:
    image: dwani/talk-server:${DWANI_TALK_SERVER_TAG:-latest}
    expose:
      - "8000"
    depends_on:
      tts-indic-server:
        condition: service_started
      vllm-server:
        condition: service_started
      agents:
        condition: service_healthy
      redis:
        condition: service_healthy
      postgres:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:8000/health')"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 5s
    environment:
      DWANI_API_BASE_URL_TTS: http://tts-indic-server:10804
      DWANI_API_BASE_URL_ASR: http://asr-indic-server:10803
      DWANI_API_BASE_URL_LLM: http://vllm-server:10802
      DWANI_LLM_MODEL: ${DWANI_LLM_MODEL:-gemma3}
      DWANI_AGENT_BASE_URL: http://agents:8081
      DWANI_REDIS_URL: ${DWANI_REDIS_URL:-redis://redis:6379/0}
      DWANI_DATABASE_URL: ${DWANI_DATABASE_URL:-postgresql+psycopg://talk:talk@postgres:5432/talk}
      DWANI_API_KEY: ${DWANI_API_KEY:-}
      DWANI_AUTH_COOKIE_NAME: ${DWANI_AUTH_COOKIE_NAME:-dwani_auth_session}
      DWANI_AUTH_SESSION_TTL_SECONDS: ${DWANI_AUTH_SESSION_TTL_SECONDS:-86400}
      DWANI_AUTH_COOKIE_SAMESITE: ${DWANI_AUTH_COOKIE_SAMESITE:-lax}
      DWANI_AUTH_COOKIE_SECURE: ${DWANI_AUTH_COOKIE_SECURE:-0}
      DWANI_LLM_API_KEY: ${DWANI_LLM_API_KEY:-dummy}
      AGENTS_API_KEY: ${AGENTS_API_KEY:-}
    extra_hosts:
      - "host.docker.internal:host-gateway"
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "2.0"
          memory: 2G

  talk-ui:
    image: dwani/talk-ux:${DWANI_TALK_UI_TAG:-latest}
    ports:
      - "80:80"
    depends_on:
      talk:
        condition: service_healthy
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: 512M
  asr-indic-server:
    image: dwani/asr-indic-server:latest
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
    ports:
      - 10803:10803
    environment:
      - HF_TOKEN
    extra_hosts:
      - "host.docker.internal:host-gateway"
    restart: unless-stopped
    deploy:
          resources:
            reservations:
              devices:
                - driver: nvidia
                  device_ids: ['0']
                  capabilities: [gpu]
  tts-indic-server:
    image: dwani/tts-indic-server:latest
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
    ports:
      - "10804:10804"
    extra_hosts:
      - "host.docker.internal:host-gateway"
    restart: unless-stopped
    environment:
      TTS_NFE_STEPS: 8
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['0']
              capabilities: [gpu]
  vllm-server:
    image: vllm/vllm-openai:latest
    command: [
      "--model", "Qwen/Qwen3-VL-8B-Instruct-FP8",
      "--served-model-name", "gemma3",
      "--host", "0.0.0.0",
      "--port", "10802",
      "--gpu-memory-utilization", "0.9",
      "--tensor-parallel-size", "1",
      "--max-model-len", "8192",
      "--disable-log-requests",
      "--dtype", "bfloat16",
      "--enable-chunked-prefill",
      "--enable-prefix-caching",
      "--max-num-batched-tokens", "8192",
      "--chat-template-content-format", "openai",
      "--enable-auto-tool-choice",
      "--tool-call-parser", "hermes"
    ]
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
    ports:
      - "10802:10802"

  agents:
    image: dwani/talk-agents:${DWANI_TALK_AGENTS_TAG:-latest}
    expose:
      - "8081"
    depends_on:
      - vllm-server
    environment:
      LITELLM_MODEL_NAME: ${LITELLM_MODEL_NAME:-openai/gemma3}
      LITELLM_API_BASE: http://vllm-server:10802/v1
      LITELLM_API_KEY: ${LITELLM_API_KEY}
      AGENTS_API_KEY: ${AGENTS_API_KEY:-}
      AGENTS_ALLOWED_ORIGINS: ${AGENTS_ALLOWED_ORIGINS:-http://localhost}
      AGENTS_REDIS_URL: ${AGENTS_REDIS_URL:-redis://redis:6379/1}
    volumes:
      - fix_my_city_data:/app/fix-my-city/data
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:8081/healthz')"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 5s
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "2.0"
          memory: 2G

  redis:
    image: redis:7-alpine
    expose:
      - "6379"
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 3
    restart: unless-stopped

  postgres:
    image: postgres:16-alpine
    environment:
      POSTGRES_USER: ${DWANI_POSTGRES_USER:-talk}
      POSTGRES_PASSWORD: ${DWANI_POSTGRES_PASSWORD:-talk}
      POSTGRES_DB: ${DWANI_POSTGRES_DB:-talk}
    expose:
      - "5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${DWANI_POSTGRES_USER:-talk} -d ${DWANI_POSTGRES_DB:-talk}"]
      interval: 10s
      timeout: 5s
      retries: 5
    volumes:
      - talk_postgres_data:/var/lib/postgresql/data
    restart: unless-stopped

volumes:
  fix_my_city_data:
  talk_postgres_data:
